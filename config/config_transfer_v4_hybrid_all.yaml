# Hybrid Transfer Learning Configuration (Prompt + LoRA)
# All 5 scenarios: InH, InF, UMi, UMa, RMa
# Method: Hybrid-M (Prompt length=100 + LoRA rank=10)
# Total trainable params: ~61,952 (Prompt: 12,800 + LoRA: ~49,152)

# Scenario definitions
scenarios:
  InH:
    channel_type: ["InH_Los", "InH_Nlos"]
    distance_ranges:
      InH_Los: [5.0, 100.0]
      InH_Nlos: [5.0, 100.0]
    wandb_proj: 'DNN_channel_estimation_InH_Hybrid_Transfer'
    saved_model_name: 'Large_estimator_v4_to_InH_hybrid_M'

  InF:
    channel_type: ["InF_Los", "InF_Nlos"]
    distance_ranges:
      InF_Los: [10.0, 100.0]
      InF_Nlos: [10.0, 100.0]
    wandb_proj: 'DNN_channel_estimation_InF_Hybrid_Transfer'
    saved_model_name: 'Large_estimator_v4_to_InF_hybrid_M'

  UMi:
    channel_type: ["UMi_Los", "UMi_Nlos"]
    distance_ranges:
      UMi_Los: [10.0, 500.0]
      UMi_Nlos: [10.0, 500.0]
    wandb_proj: 'DNN_channel_estimation_UMi_Hybrid_Transfer'
    saved_model_name: 'Large_estimator_v4_to_UMi_hybrid_M'

  UMa:
    channel_type: ["UMa_Los", "UMa_Nlos"]
    distance_ranges:
      UMa_Los: [10.0, 10000.0]
      UMa_Nlos: [10.0, 10000.0]
    wandb_proj: 'DNN_channel_estimation_UMa_Hybrid_Transfer'
    saved_model_name: 'Large_estimator_v4_to_UMa_hybrid_M'

  RMa:
    channel_type: ["RMa_Los_10000", "RMa_Nlos_10000"]
    distance_ranges:
      RMa_Los: [10.0, 10000.0]
      RMa_Nlos: [10.0, 10000.0]
    wandb_proj: 'DNN_channel_estimation_RMa_Hybrid_Transfer'
    saved_model_name: 'Large_estimator_v4_to_RMa_hybrid_M'

# Common dataset settings
dataset:
  batch_size: 32
  noise_spectral_density: -174.0  # dBm/Hz
  subcarrier_spacing: 120.0  # kHz
  transmit_power: 30.0  # dBm
  carrier_freq: 28.0  # GHz
  mod_order: 64
  ref_conf_dict:
    'dmrs': [0, 3072, 6]
  fft_size: 4096
  num_guard_subcarriers: 1024
  num_symbol: 14
  cp_length: 590  # ns
  max_random_tap_delay_cp_proportion: 0.2
  rnd_seed: 0
  num_workers: 0
  is_phase_noise: False
  is_channel: True
  is_noise: True

# Training settings
training:
  lr: 0.0001
  weight_decay: 0.000001
  max_norm: 1.0
  num_iter: 100000  # 100K iterations
  logging_step: 50
  evaluation_step: 2000
  evaluation_batch_size: 4
  pn_train_start_iter: 50000
  use_scheduler: True
  num_warmup_steps: 0
  use_early_stopping: false
  device: 'cuda:0'
  use_wandb: True
  pretrained_model_name: 'Large_estimator_v4_base_extended_final'
  ch_loss_weight: 1
  num_freeze_layers: 0
  model_load_mode: 'pretrained'
  load_model_path: ''
  model_save_step: 20000  # Save every 20K iterations

# Channel estimation model settings
ch_estimation:
  cond:
    length: 3072
    in_channels: 2
    step_size: 12
    steps_per_token: 1
  transformer:
    length: 3072
    channels: 2
    num_layers: 4
    d_model: 128
    n_token: 256
    n_head: 8
    dim_feedforward: 1024
    dropout: 0.1
    activation: 'relu'

  # Prompt Learning configuration
  prompt:
    prompt_length: 100  # Hybrid-M: 100 tokens (12,800 params)
    freeze_base_model: true  # Keep base model frozen

  # LoRA configuration
  peft:
    peft_type: LORA
    r: 10  # Hybrid-M: rank=10 (~49,152 params)
    lora_alpha: 16
    target_modules: ["mha_q_proj", "mha_v_proj", "ffnn_linear1"]
    lora_dropout: 0.05

auto_upload:
  enabled: false
  repository: "https://github.com/Joowonoil/vastai_trained_model.git"
  include_config: true
  include_training_log: true
